{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hyper-parameter-Tuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPEvM+bu1JvyaOBQgnIuntn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abotalebmostafa11/Hyper-parameter-Tuning/blob/main/Hyper_parameter_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "F5u4aowKCcvA",
        "outputId": "012bde9a-3a33-44ee-8cd9-76804137efe1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f5d137c5-d3dc-4bf2-ae4d-0917b1470245\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f5d137c5-d3dc-4bf2-ae4d-0917b1470245\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Data.csv to Data.csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM for Forecasting Covid 19 problem In Russian Fderation\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "from tensorflow.keras.layers import Dense,RepeatVector,LSTM,Dropout,GRU,Dense,SimpleRNN,Embedding \n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping\n",
        "from tensorflow.keras.layers import Flatten, Conv1D, MaxPooling1D\n",
        "from keras.preprocessing.sequence import TimeseriesGenerator\n",
        "from tensorflow.keras.layers import Bidirectional, Dropout\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.layers import LSTM, TimeDistributed\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Dense, Softmax\n",
        "from sklearn.metrics import r2_score\n",
        "from keras.models import Sequential\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas import read_csv\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import numpy\n",
        "import math"
      ],
      "metadata": {
        "id": "7cNFNY1GC8HX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Name=input('Enter The Name Of The Data File:')\n",
        "Use_Percent = input('Do You Want to Use Percent For Testing Data? True or false?').lower() == 'true'\n",
        "if Use_Percent:\n",
        "    train_Percent = float(input('Enter the percentage of train data:'))\n",
        "else:\n",
        "    Testing_Rows = int(input('Enter The Number Of Testing Rows:'))\n",
        "Forecasting_days = int(input('Enter the number of forecasts you want to make:'))\n",
        "create_new_model = input('Do You Want to Create a New Model? Please choose True or False:')\n",
        "Time=(input('Enter the Type of your data (days, weeks, months, years):'))\n",
        "Implementation=(input('Enter Name of your case \"For example Infection Cases\":')) \n",
        "plot_title=(input('Enter the plot title for example \"Modelling Covid 19 in Russian Federation\":')) \n",
        "Model_Name=(input('Put name of model, please select (lstm - lstm window- stacked lstm - convlstms -  bilstm - stacked bilstm ) :'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qTSbS2BDbHv",
        "outputId": "a8556469-d7e4-4b9b-d6a1-4733c11268c8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter The Name Of The Data File:Data.csv\n",
            "Do You Want to Use Percent For Testing Data? True or false?True\n",
            "Enter the percentage of train data:0.95\n",
            "Enter the number of forecasts you want to make:30\n",
            "Do You Want to Create a New Model? Please choose True or False:True\n",
            "Enter the Type of your data (days, weeks, months, years):days\n",
            "Enter Name of your case \"For example Infection Cases\":Infection cases in Russian federation\n",
            "Enter the plot title for example \"Modelling Covid 19 in Russian Federation\":Modelling and forecasting daily Covid 19 in Russian Federation using BILSTM\n",
            "Put name of model, please select (lstm - lstm window- stacked lstm - convlstms -  bilstm - stacked bilstm ) :bilstm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert an array of values into a dataset matrix\n",
        "def create_dataset(dataset, look_back=1):\n",
        "\tdataX, dataY = [], []\n",
        "\tfor i in range(len(dataset)-look_back-1):\n",
        "\t\ta = dataset[i:(i+look_back), 0]\n",
        "\t\tdataX.append(a)\n",
        "\t\tdataY.append(dataset[i + look_back, 0])\n",
        "\treturn numpy.array(dataX), numpy.array(dataY)\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "# load the dataset\n",
        "dataframe = read_csv(Data_Name, usecols=[1], engine='python')\n",
        "dataset = dataframe.values\n",
        "dataset = dataset.astype('float32')\n",
        "# normalize the dataset\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "dataset = scaler.fit_transform(dataset)\n",
        "# split into train and test sets\n",
        "if Use_Percent:\n",
        "    train_size = int(len(dataset) * (train_Percent))\n",
        "    test_size = len(dataset) - train_size\n",
        "    pass\n",
        "else:\n",
        "    train_size = len(dataset) - Testing_Rows\n",
        "    test_size = Testing_Rows\n",
        "    pass\n",
        "\n",
        "#train_size = int(len(dataset) * train_Percent)\n",
        "#test_size = len(dataset) - train_size\n",
        "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
        "# reshape into X=t and Y=t+1"
      ],
      "metadata": {
        "id": "NJsWG30qGQwM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "look_back=int(input('LSTM for Regression Using the Window Method (increasing the look_back argument from 1 to 3)[№ look back]:'))\n",
        "trainX, trainY = create_dataset(train, look_back)\n",
        "testX, testY = create_dataset(test, look_back)\n",
        "# reshape input to be [samples, time steps, features]\n",
        "#trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "#testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
        "trainX = numpy.reshape(trainX, (trainX.shape[0], look_back, 1))\n",
        "testX = numpy.reshape(testX, (testX.shape[0], look_back, 1))\n",
        "trainY = trainY.T\n",
        "testY = testY.T\n",
        "print(f'trainX = {trainX.shape}')\n",
        "print(f'trainY = {trainY.shape}')\n",
        "print(f'testX = {testX.shape}')\n",
        "print(f'testY = {testY.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iVVypRvGXLb",
        "outputId": "b4d79ff5-75ad-44dd-f7ac-a5eca9cc148f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM for Regression Using the Window Method (increasing the look_back argument from 1 to 3)[№ look back]:1\n",
            "trainX = (690, 1, 1)\n",
            "trainY = (690,)\n",
            "testX = (35, 1, 1)\n",
            "testY = (35,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if create_new_model:\n",
        "    model = Sequential()\n",
        "    n_input = 12\n",
        "    n_features= 1\n",
        "    generator = TimeseriesGenerator(dataset, dataset, length=n_input, batch_size=n_features)\n",
        "    early_stop = EarlyStopping(monitor = \"loss\", mode = \"min\", patience = 7)\n",
        "    #for layer in Model_Name:\n",
        "    if Model_Name.lower() == \"bilstm\":\n",
        "        # create and fit the BILSTM for Regression network (bilstm)\n",
        "        print('Machine learning software = => create and fit the BILSTM for Regression network...')\n",
        "        N_epochs = int(input('Enter the number of epochs:'))\n",
        "        inputloss_function =(input('Enter the Loss Function:'))\n",
        "        inputoptimizer_name =(input('Enter the Optimizer Name:'))\n",
        "        activation=(input('Enter the activation function name:'))\n",
        "        NN=int(input('Enter the number of neurans:'))\n",
        "        model.add(Bidirectional(LSTM(NN,activation=activation, input_shape=(look_back,1))))\n",
        "        model.add(Dense(1))\n",
        "    if  Model_Name.lower() == \"stacked bilstm\":\n",
        "        # create and fit the BILSTM stack network (stacked bilstm)\n",
        "        print('Machine learning software = => create and fit the stacked bilstm for Regression network...')\n",
        "        N_epochs = int(input('Enter the number of epochs:'))\n",
        "        inputloss_function =(input('Enter the Loss Function:'))\n",
        "        inputoptimizer_name =(input('Enter the Optimizer Name:'))\n",
        "        activation=(input('Enter the activation function name:'))\n",
        "        NN=int(input('Enter the number of neurans:'))\n",
        "        model.add(Bidirectional(LSTM(NN, return_sequences=True, activation=activation, input_shape=(look_back,1))))\n",
        "        model.add(Bidirectional(LSTM(NN, return_sequences=True, activation=activation, input_shape=(look_back, 1))))\n",
        "        model.add(Bidirectional(LSTM(20)))\n",
        "        model.add(Dense(1))\n",
        "    if  Model_Name.lower() == \"convlstms\":\n",
        "        print('Machine learning software = => create and fit the convlstms for Regression network...')\n",
        "        look_back=30\n",
        "        trainX, trainY = create_dataset(train, look_back)\n",
        "        testX, testY = create_dataset(test, look_back)\n",
        "        # reshape input to be [samples, time steps, features]\n",
        "        trainX = trainX.reshape(trainX.shape[0], trainX.shape[1],1)\n",
        "        testX = testX.reshape(testX.shape[0], testX.shape[1],1)\n",
        "        trainY = trainY.T\n",
        "        testY = testY.T\n",
        "        print(f'trainX = {trainX.shape}')\n",
        "        print(f'trainY = {trainY.shape}')\n",
        "        print(f'testX = {testX.shape}')\n",
        "        print(f'testY = {testY.shape}')\n",
        "        # create and fit the convlstms network (convlstms)\n",
        "        N_epochs = int(input('Enter the number of epochs:'))\n",
        "        inputloss_function =(input('Enter the Loss Function:'))\n",
        "        inputoptimizer_name =(input('Enter the Optimizer Name:'))\n",
        "        activation=(input('Enter the activation function name:'))\n",
        "        model.add(Conv1D(filters=256, kernel_size=2, activation=activation, input_shape=(look_back,1)))\n",
        "        model.add(Conv1D(filters=128, kernel_size=2, activation=activation))\n",
        "        model.add(MaxPooling1D(pool_size=2))\n",
        "        model.add(Flatten())\n",
        "        model.add(RepeatVector(30))\n",
        "        model.add(LSTM(units=100, return_sequences=True, activation=activation))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(LSTM(units=100, return_sequences=True, activation=activation))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(LSTM(units=100, return_sequences=True, activation=activation))\n",
        "        model.add(LSTM(units=100, return_sequences=True, activation=activation))\n",
        "        model.add(Bidirectional(LSTM(128, activation=activation)))\n",
        "        model.add(Dense(100, activation=activation))\n",
        "        model.add(Dense(1))\n",
        "    if  Model_Name.lower() == \"conv\":\n",
        "        print('Machine learning software = => create and fit the convlstms for Regression network...')\n",
        "        look_back=30\n",
        "        trainX, trainY = create_dataset(train, look_back)\n",
        "        testX, testY = create_dataset(test, look_back)\n",
        "        # reshape input to be [samples, time steps, features]\n",
        "        trainX = trainX.reshape(trainX.shape[0], trainX.shape[1],1)\n",
        "        testX = testX.reshape(testX.shape[0], testX.shape[1],1)\n",
        "        trainY = trainY.T\n",
        "        testY = testY.T\n",
        "        print(f'trainX = {trainX.shape}')\n",
        "        print(f'trainY = {trainY.shape}')\n",
        "        print(f'testX = {testX.shape}')\n",
        "        print(f'testY = {testY.shape}')\n",
        "        # create and fit the convlstms network (convlstm)\n",
        "        N_epochs = int(input('Enter the number of epochs:'))\n",
        "        inputloss_function =(input('Enter the Loss Function:'))\n",
        "        inputoptimizer_name =(input('Enter the Optimizer Name:'))\n",
        "        activation=(input('Enter the activation function name:'))\n",
        "        model.add(Conv1D(filters=256, kernel_size=2, activation=activation, input_shape=(look_back,1)))\n",
        "        model.add(MaxPooling1D(pool_size=2))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(100, activation=activation))\n",
        "        model.add(Dense(1))\n",
        "    if  Model_Name.lower() == \"convlstms1\":\n",
        "        print('Machine learning software = => create and fit the convlstms for Regression network...')\n",
        "        look_back=30\n",
        "        trainX, trainY = create_dataset(train, look_back)\n",
        "        testX, testY = create_dataset(test, look_back)\n",
        "        # reshape input to be [samples, time steps, features]\n",
        "        trainX = trainX.reshape(trainX.shape[0], trainX.shape[1],1)\n",
        "        testX = testX.reshape(testX.shape[0], testX.shape[1],1)\n",
        "        trainY = trainY.T\n",
        "        testY = testY.T\n",
        "        print(f'trainX = {trainX.shape}')\n",
        "        print(f'trainY = {trainY.shape}')\n",
        "        print(f'testX = {testX.shape}')\n",
        "        print(f'testY = {testY.shape}')\n",
        "        # create and fit the convlstms network (convlstms)\n",
        "        N_epochs = int(input('Enter the number of epochs:'))\n",
        "        inputloss_function =(input('Enter the Loss Function:'))\n",
        "        inputoptimizer_name =(input('Enter the Optimizer Name:'))\n",
        "        activation=(input('Enter the activation function name:'))\n",
        "        model.add(Conv1D(filters=256, kernel_size=2, activation=activation, input_shape=(look_back,1)))\n",
        "        model.add(Conv1D(filters=128, kernel_size=2, activation=activation))\n",
        "        model.add(MaxPooling1D(pool_size=2))\n",
        "        model.add(Flatten())\n",
        "        model.add(RepeatVector(30))\n",
        "        model.add(LSTM(units=100, return_sequences=True, activation=activation))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(LSTM(units=100, return_sequences=True, activation=activation))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(LSTM(units=100, return_sequences=True, activation=activation))\n",
        "        model.add(Bidirectional(LSTM(128, activation=activation)))\n",
        "        model.add(Dense(100, activation=activation))\n",
        "        model.add(Dense(1))\n",
        "    if  Model_Name.lower() == \"convbilstms\":\n",
        "        print('Machine learning software = => create and fit the convbilstms for Regression network...')\n",
        "        look_back=30\n",
        "        trainX, trainY = create_dataset(train, look_back)\n",
        "        testX, testY = create_dataset(test, look_back)\n",
        "        # reshape input to be [samples, time steps, features]\n",
        "        trainX = trainX.reshape(trainX.shape[0], trainX.shape[1],1)\n",
        "        testX = testX.reshape(testX.shape[0], testX.shape[1],1)\n",
        "        trainY = trainY.T\n",
        "        testY = testY.T\n",
        "        print(f'trainX = {trainX.shape}')\n",
        "        print(f'trainY = {trainY.shape}')\n",
        "        print(f'testX = {testX.shape}')\n",
        "        print(f'testY = {testY.shape}')\n",
        "        # create and fit the convlstms network (convlstms)\n",
        "        N_epochs = int(input('Enter the number of epochs:'))\n",
        "        inputloss_function =(input('Enter the Loss Function:'))\n",
        "        inputoptimizer_name =(input('Enter the Optimizer Name:'))\n",
        "        activation=(input('Enter the activation function name:'))\n",
        "        model.add(Conv1D(filters=256, kernel_size=2, activation=activation, input_shape=(look_back,1)))\n",
        "        model.add(Conv1D(filters=128, kernel_size=2, activation=activation))\n",
        "        model.add(MaxPooling1D(pool_size=2))\n",
        "        model.add(Flatten())\n",
        "        model.add(RepeatVector(30))\n",
        "        model.add(Bidirectional(LSTM(units=100, return_sequences=True, activation=activation)))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(Bidirectional(LSTM(units=100, return_sequences=True, activation=activation)))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(Bidirectional(LSTM(units=100, return_sequences=True, activation=activation)))\n",
        "        model.add(Bidirectional(LSTM(units=100, return_sequences=True, activation=activation)))\n",
        "        model.add(LSTM(128, activation=activation))\n",
        "        model.add(Dense(100, activation=activation))\n",
        "        model.add(Dense(1))       \n",
        "    if  Model_Name.lower() == \"convbilstms1\":\n",
        "        print('Machine learning software = => create and fit the convbilstms for Regression network...')\n",
        "        look_back=30\n",
        "        trainX, trainY = create_dataset(train, look_back)\n",
        "        testX, testY = create_dataset(test, look_back)\n",
        "        # reshape input to be [samples, time steps, features]\n",
        "        trainX = trainX.reshape(trainX.shape[0], trainX.shape[1],1)\n",
        "        testX = testX.reshape(testX.shape[0], testX.shape[1],1)\n",
        "        trainY = trainY.T\n",
        "        testY = testY.T\n",
        "        print(f'trainX = {trainX.shape}')\n",
        "        print(f'trainY = {trainY.shape}')\n",
        "        print(f'testX = {testX.shape}')\n",
        "        print(f'testY = {testY.shape}')\n",
        "        # create and fit the convlstms network (convlstms)\n",
        "        N_epochs = int(input('Enter the number of epochs:'))\n",
        "        inputloss_function =(input('Enter the Loss Function:'))\n",
        "        inputoptimizer_name =(input('Enter the Optimizer Name:'))\n",
        "        activation=(input('Enter the activation function name:'))\n",
        "        model.add(Conv1D(filters=256, kernel_size=2, activation=activation, input_shape=(look_back,1)))\n",
        "        model.add(Conv1D(filters=128, kernel_size=2, activation=activation))\n",
        "        model.add(MaxPooling1D(pool_size=2))\n",
        "        model.add(Flatten())\n",
        "        model.add(RepeatVector(30))\n",
        "        model.add(Bidirectional(LSTM(units=100, return_sequences=True, activation=activation)))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(Bidirectional(LSTM(units=100, return_sequences=True, activation=activation)))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(Bidirectional(LSTM(units=100, return_sequences=True, activation=activation)))\n",
        "        model.add(LSTM(128, activation=activation))\n",
        "        model.add(Dense(100, activation=activation))\n",
        "        model.add(Dense(1)) \n",
        "    \n",
        "    if  Model_Name.lower() == \"lstm window\":\n",
        "        # create and fit the LSTM for Regression Using the Window Method network (lstm window)\n",
        "        print('Machine learning software = => create and fit the lstm window for Regression network...')\n",
        "        N_epochs = int(input('Enter the number of epochs:'))\n",
        "        inputloss_function =(input('Enter the Loss Function:'))\n",
        "        inputoptimizer_name =(input('Enter the Optimizer Name:'))\n",
        "        activation=(input('Enter the activation function name:'))\n",
        "        NN=int(input('Enter the number of neurans:'))\n",
        "        model.add(LSTM(NN,activation=activation, input_shape=(look_back, 1)))\n",
        "        model.add(Dense(1))\n",
        "    if  Model_Name.lower() == \"lstm\":\n",
        "        print('Machine learning software = => create and fit the lstm for Regression network...')\n",
        "        # create and fit the LSTM for Regression network (lstm)\n",
        "        N_epochs = int(input('Enter the number of epochs:'))\n",
        "        inputloss_function =(input('Enter the Loss Function:'))\n",
        "        inputoptimizer_name =(input('Enter the Optimizer Name:'))\n",
        "        activation=(input('Enter the activation function name:'))\n",
        "        NN=int(input('Enter the number of neurans:'))\n",
        "        model.add(LSTM(NN,activation=activation, input_shape=(look_back,1)))\n",
        "        model.add(Dense(1))\n",
        "    if  Model_Name.lower() == \"stacked lstm\":\n",
        "        print('Machine learning software = => create and fit the stacked lstm for Regression network...')\n",
        "        # create and fit the Stacked LSTM for Regression network (stacked lstm)\n",
        "        N_epochs = int(input('Enter the number of epochs:'))\n",
        "        inputloss_function =(input('Enter the Loss Function:'))\n",
        "        inputoptimizer_name =(input('Enter the Optimizer Name:'))\n",
        "        activation=(input('Enter the activation function name:'))\n",
        "        NN=int(input('Enter the number of neurans:'))\n",
        "        model.add(LSTM(NN, return_sequences=True, activation=activation, input_shape=(look_back, 1)))\n",
        "        model.add(LSTM(NN, return_sequences=True, activation=activation, input_shape=(look_back, 1)))\n",
        "        model.add(LSTM(20))\n",
        "        model.add(Dense(1))     \n",
        "\n",
        "    model.compile(loss=inputloss_function, optimizer=inputoptimizer_name,metrics=['accuracy'])\n",
        "    history =model.fit(trainX, trainY, epochs=N_epochs, batch_size=1, verbose=True,validation_split=0.2, shuffle=True)\n",
        "    print(history.history.keys())\n",
        "    #  \"Accuracy\"\n",
        "    fig = plt.figure(figsize=(12,4))\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'validation'], loc='upper right')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "    fig.savefig('Accuracy.png', dpi=fig.dpi, facecolor='white', bbox_inches='tight')\n",
        "    # \"Loss\" # plot train and validation loss\n",
        "    fig = plt.figure(figsize=(12,4))\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model train vs validation loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'validation'], loc='upper right')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "    fig.savefig('plot train and validation loss.png', dpi=fig.dpi, facecolor='white', bbox_inches='tight')\n",
        "    model.save(Model_Name)\n",
        "else:\n",
        "    model = load_model(Model_Name)\n",
        "    model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qv6cPhbYGclk",
        "outputId": "5fae624b-cf79-4830-edac-cf01f2102930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine learning software = => create and fit the BILSTM for Regression network...\n",
            "Enter the number of epochs:1000\n",
            "Enter the Loss Function:mse\n",
            "Enter the Optimizer Name:Ftrl\n",
            "Enter the activation function name:elu\n",
            "Enter the number of neurans:100\n",
            "Epoch 1/1000\n",
            "552/552 [==============================] - 6s 5ms/step - loss: 0.0903 - accuracy: 0.1250 - val_loss: 0.4098 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/1000\n",
            "552/552 [==============================] - 2s 4ms/step - loss: 0.0820 - accuracy: 0.1250 - val_loss: 0.3932 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/1000\n",
            "552/552 [==============================] - 2s 4ms/step - loss: 0.0772 - accuracy: 0.1250 - val_loss: 0.3811 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/1000\n",
            "552/552 [==============================] - 2s 4ms/step - loss: 0.0737 - accuracy: 0.1250 - val_loss: 0.3714 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/1000\n",
            "552/552 [==============================] - 2s 4ms/step - loss: 0.0709 - accuracy: 0.1250 - val_loss: 0.3632 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/1000\n",
            "552/552 [==============================] - 2s 4ms/step - loss: 0.0685 - accuracy: 0.1250 - val_loss: 0.3561 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/1000\n",
            "552/552 [==============================] - 2s 4ms/step - loss: 0.0665 - accuracy: 0.1250 - val_loss: 0.3497 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/1000\n",
            "552/552 [==============================] - 2s 4ms/step - loss: 0.0647 - accuracy: 0.1250 - val_loss: 0.3440 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/1000\n",
            "552/552 [==============================] - 2s 4ms/step - loss: 0.0632 - accuracy: 0.1250 - val_loss: 0.3388 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/1000\n",
            "552/552 [==============================] - 2s 3ms/step - loss: 0.0618 - accuracy: 0.1250 - val_loss: 0.3340 - val_accuracy: 0.0000e+00\n",
            "Epoch 11/1000\n",
            "552/552 [==============================] - 2s 4ms/step - loss: 0.0605 - accuracy: 0.1250 - val_loss: 0.3296 - val_accuracy: 0.0000e+00\n",
            "Epoch 12/1000\n",
            "552/552 [==============================] - 3s 5ms/step - loss: 0.0594 - accuracy: 0.1250 - val_loss: 0.3255 - val_accuracy: 0.0000e+00\n",
            "Epoch 13/1000\n",
            "552/552 [==============================] - 3s 6ms/step - loss: 0.0583 - accuracy: 0.1250 - val_loss: 0.3217 - val_accuracy: 0.0000e+00\n",
            "Epoch 14/1000\n",
            "552/552 [==============================] - 2s 3ms/step - loss: 0.0573 - accuracy: 0.1250 - val_loss: 0.3180 - val_accuracy: 0.0000e+00\n",
            "Epoch 15/1000\n",
            "552/552 [==============================] - 2s 4ms/step - loss: 0.0564 - accuracy: 0.1250 - val_loss: 0.3146 - val_accuracy: 0.0000e+00\n",
            "Epoch 16/1000\n",
            "552/552 [==============================] - 2s 4ms/step - loss: 0.0556 - accuracy: 0.1250 - val_loss: 0.3114 - val_accuracy: 0.0000e+00\n",
            "Epoch 17/1000\n",
            "552/552 [==============================] - 2s 3ms/step - loss: 0.0548 - accuracy: 0.1250 - val_loss: 0.3084 - val_accuracy: 0.0000e+00\n",
            "Epoch 18/1000\n",
            "552/552 [==============================] - 2s 4ms/step - loss: 0.0541 - accuracy: 0.1250 - val_loss: 0.3055 - val_accuracy: 0.0000e+00\n",
            "Epoch 19/1000\n",
            "552/552 [==============================] - 2s 3ms/step - loss: 0.0534 - accuracy: 0.1250 - val_loss: 0.3027 - val_accuracy: 0.0000e+00\n",
            "Epoch 20/1000\n",
            "552/552 [==============================] - 2s 4ms/step - loss: 0.0528 - accuracy: 0.1250 - val_loss: 0.3001 - val_accuracy: 0.0000e+00\n",
            "Epoch 21/1000\n",
            "552/552 [==============================] - 2s 4ms/step - loss: 0.0522 - accuracy: 0.1250 - val_loss: 0.2976 - val_accuracy: 0.0000e+00\n",
            "Epoch 22/1000\n",
            "552/552 [==============================] - 2s 3ms/step - loss: 0.0517 - accuracy: 0.1250 - val_loss: 0.2952 - val_accuracy: 0.0000e+00\n",
            "Epoch 23/1000\n",
            "552/552 [==============================] - 2s 4ms/step - loss: 0.0511 - accuracy: 0.1250 - val_loss: 0.2928 - val_accuracy: 0.0000e+00\n",
            "Epoch 24/1000\n",
            "552/552 [==============================] - 2s 4ms/step - loss: 0.0506 - accuracy: 0.1250 - val_loss: 0.2906 - val_accuracy: 0.0000e+00\n",
            "Epoch 25/1000\n",
            "552/552 [==============================] - 2s 3ms/step - loss: 0.0501 - accuracy: 0.1250 - val_loss: 0.2884 - val_accuracy: 0.0000e+00\n",
            "Epoch 26/1000\n",
            "552/552 [==============================] - 2s 3ms/step - loss: 0.0497 - accuracy: 0.1250 - val_loss: 0.2863 - val_accuracy: 0.0000e+00\n",
            "Epoch 27/1000\n",
            "552/552 [==============================] - 2s 3ms/step - loss: 0.0493 - accuracy: 0.1250 - val_loss: 0.2843 - val_accuracy: 0.0000e+00\n",
            "Epoch 28/1000\n",
            "552/552 [==============================] - 2s 4ms/step - loss: 0.0488 - accuracy: 0.1250 - val_loss: 0.2823 - val_accuracy: 0.0000e+00\n",
            "Epoch 29/1000\n",
            "552/552 [==============================] - 2s 3ms/step - loss: 0.0484 - accuracy: 0.1250 - val_loss: 0.2804 - val_accuracy: 0.0000e+00\n",
            "Epoch 30/1000\n",
            "552/552 [==============================] - 2s 4ms/step - loss: 0.0481 - accuracy: 0.1250 - val_loss: 0.2785 - val_accuracy: 0.0000e+00\n",
            "Epoch 31/1000\n",
            "552/552 [==============================] - 2s 3ms/step - loss: 0.0477 - accuracy: 0.1250 - val_loss: 0.2766 - val_accuracy: 0.0000e+00\n",
            "Epoch 32/1000\n",
            "552/552 [==============================] - 2s 4ms/step - loss: 0.0473 - accuracy: 0.1250 - val_loss: 0.2748 - val_accuracy: 0.0000e+00\n",
            "Epoch 33/1000\n",
            "552/552 [==============================] - 2s 4ms/step - loss: 0.0470 - accuracy: 0.1250 - val_loss: 0.2730 - val_accuracy: 0.0000e+00\n",
            "Epoch 34/1000\n",
            "552/552 [==============================] - 2s 3ms/step - loss: 0.0466 - accuracy: 0.1250 - val_loss: 0.2712 - val_accuracy: 0.0000e+00\n",
            "Epoch 35/1000\n",
            "552/552 [==============================] - 2s 4ms/step - loss: 0.0463 - accuracy: 0.1250 - val_loss: 0.2693 - val_accuracy: 0.0000e+00\n",
            "Epoch 36/1000\n",
            "552/552 [==============================] - 2s 4ms/step - loss: 0.0460 - accuracy: 0.1250 - val_loss: 0.2675 - val_accuracy: 0.0000e+00\n",
            "Epoch 37/1000\n",
            "552/552 [==============================] - 2s 3ms/step - loss: 0.0456 - accuracy: 0.1250 - val_loss: 0.2657 - val_accuracy: 0.0000e+00\n",
            "Epoch 38/1000\n",
            "552/552 [==============================] - 2s 4ms/step - loss: 0.0453 - accuracy: 0.1250 - val_loss: 0.2638 - val_accuracy: 0.0000e+00\n",
            "Epoch 39/1000\n",
            "552/552 [==============================] - 2s 3ms/step - loss: 0.0450 - accuracy: 0.1250 - val_loss: 0.2619 - val_accuracy: 0.0000e+00\n",
            "Epoch 40/1000\n",
            "552/552 [==============================] - 2s 3ms/step - loss: 0.0447 - accuracy: 0.1250 - val_loss: 0.2599 - val_accuracy: 0.0000e+00\n",
            "Epoch 41/1000\n",
            "552/552 [==============================] - 2s 4ms/step - loss: 0.0443 - accuracy: 0.1250 - val_loss: 0.2579 - val_accuracy: 0.0000e+00\n",
            "Epoch 42/1000\n",
            "552/552 [==============================] - 2s 3ms/step - loss: 0.0440 - accuracy: 0.1250 - val_loss: 0.2557 - val_accuracy: 0.0000e+00\n",
            "Epoch 43/1000\n",
            "552/552 [==============================] - 2s 4ms/step - loss: 0.0436 - accuracy: 0.1250 - val_loss: 0.2536 - val_accuracy: 0.0000e+00\n",
            "Epoch 44/1000\n",
            "552/552 [==============================] - 2s 4ms/step - loss: 0.0433 - accuracy: 0.1250 - val_loss: 0.2513 - val_accuracy: 0.0000e+00\n",
            "Epoch 45/1000\n",
            "552/552 [==============================] - 2s 3ms/step - loss: 0.0429 - accuracy: 0.1250 - val_loss: 0.2489 - val_accuracy: 0.0000e+00\n",
            "Epoch 46/1000\n",
            "552/552 [==============================] - 2s 4ms/step - loss: 0.0425 - accuracy: 0.1250 - val_loss: 0.2465 - val_accuracy: 0.0000e+00\n",
            "Epoch 47/1000\n",
            "552/552 [==============================] - 2s 3ms/step - loss: 0.0422 - accuracy: 0.1250 - val_loss: 0.2439 - val_accuracy: 0.0000e+00\n",
            "Epoch 48/1000\n",
            "552/552 [==============================] - 2s 4ms/step - loss: 0.0418 - accuracy: 0.1250 - val_loss: 0.2412 - val_accuracy: 0.0000e+00\n",
            "Epoch 49/1000\n",
            "552/552 [==============================] - 2s 4ms/step - loss: 0.0414 - accuracy: 0.1250 - val_loss: 0.2385 - val_accuracy: 0.0000e+00\n",
            "Epoch 50/1000\n",
            "552/552 [==============================] - 2s 3ms/step - loss: 0.0410 - accuracy: 0.1250 - val_loss: 0.2356 - val_accuracy: 0.0000e+00\n",
            "Epoch 51/1000\n",
            "509/552 [==========================>...] - ETA: 0s - loss: 0.0410 - accuracy: 0.1218"
          ]
        }
      ]
    }
  ]
}